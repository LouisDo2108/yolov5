YOLOv5 ðŸš€ v7.0-72-g064365d Python-3.8.15 torch-1.12.1 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 11017MiB)

Fusing layers... 
Model summary: 157 layers, 7026307 parameters, 0 gradients, 15.8 GFLOPs
YOLOv5 ðŸš€ v7.0-72-g064365d Python-3.8.15 torch-1.12.1 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 11017MiB)

Fusing layers... 
Model summary: 157 layers, 7026307 parameters, 0 gradients, 15.8 GFLOPs
Epoch 0/99, current lr=0.0001
Copied best model weights!
train loss: 0.929945, val loss: 0.598602, f1-score-micro: 81.62, f1-score-weighted: 69.04
----------
Epoch 1/99, current lr=0.0001
train loss: 0.656030, val loss: 0.699807, f1-score-micro: 52.86, f1-score-weighted: 53.13
----------
Epoch 2/99, current lr=0.0001
train loss: 0.606504, val loss: 0.613807, f1-score-micro: 80.95, f1-score-weighted: 69.80
----------
Epoch 3/99, current lr=0.0001
Copied best model weights!
train loss: 0.500711, val loss: 0.551628, f1-score-micro: 81.10, f1-score-weighted: 72.64
----------
Epoch 4/99, current lr=0.0001
Copied best model weights!
train loss: 0.418754, val loss: 0.469009, f1-score-micro: 84.55, f1-score-weighted: 82.58
----------
Epoch 5/99, current lr=0.0001
Copied best model weights!
train loss: 0.296281, val loss: 0.417920, f1-score-micro: 84.00, f1-score-weighted: 81.13
----------
Epoch 6/99, current lr=0.0001
train loss: 0.216355, val loss: 0.508583, f1-score-micro: 74.50, f1-score-weighted: 75.30
----------
Epoch 7/99, current lr=0.0001
Copied best model weights!
train loss: 0.174831, val loss: 0.369066, f1-score-micro: 85.17, f1-score-weighted: 85.07
----------
Epoch 8/99, current lr=0.0001
train loss: 0.146611, val loss: 0.411033, f1-score-micro: 81.43, f1-score-weighted: 81.25
----------
Epoch 9/99, current lr=0.0001
train loss: 0.092344, val loss: 0.421193, f1-score-micro: 84.50, f1-score-weighted: 84.29
----------
Epoch 10/99, current lr=0.0001
train loss: 0.062346, val loss: 0.463843, f1-score-micro: 84.02, f1-score-weighted: 83.75
----------
Epoch 11/99, current lr=0.0001
train loss: 0.044183, val loss: 0.488477, f1-score-micro: 81.90, f1-score-weighted: 82.05
----------
Epoch 12/99, current lr=0.0001
train loss: 0.057167, val loss: 0.588520, f1-score-micro: 83.74, f1-score-weighted: 82.78
----------
Epoch 13/99, current lr=0.0001
train loss: 0.025808, val loss: 0.663088, f1-score-micro: 82.88, f1-score-weighted: 81.90
----------
Epoch 14/99, current lr=0.0001
train loss: 0.024977, val loss: 0.562088, f1-score-micro: 86.61, f1-score-weighted: 86.10
----------
Epoch 15/99, current lr=0.0001
train loss: 0.016506, val loss: 0.699383, f1-score-micro: 79.90, f1-score-weighted: 80.07
----------
Epoch 16/99, current lr=0.0001
train loss: 0.009574, val loss: 0.673608, f1-score-micro: 83.17, f1-score-weighted: 83.01
----------
Epoch 17/99, current lr=0.0001
train loss: 0.008859, val loss: 0.604274, f1-score-micro: 83.03, f1-score-weighted: 83.16
----------
Epoch 18/99, current lr=0.0001
train loss: 0.008571, val loss: 0.766778, f1-score-micro: 84.71, f1-score-weighted: 84.45
----------
Epoch 19/99, current lr=0.0001
train loss: 0.006496, val loss: 0.721769, f1-score-micro: 81.25, f1-score-weighted: 81.33
----------
Epoch 20/99, current lr=0.0001
train loss: 0.005959, val loss: 0.751839, f1-score-micro: 84.26, f1-score-weighted: 83.97
----------
Epoch 21/99, current lr=0.0001
train loss: 0.004966, val loss: 0.772964, f1-score-micro: 81.34, f1-score-weighted: 81.39
----------
Epoch 22/99, current lr=0.0001
train loss: 0.005421, val loss: 0.808711, f1-score-micro: 83.95, f1-score-weighted: 83.83
----------
Epoch 23/99, current lr=0.0001
train loss: 0.002758, val loss: 0.810360, f1-score-micro: 83.12, f1-score-weighted: 82.83
----------
Epoch 24/99, current lr=0.0001
train loss: 0.007898, val loss: 0.782838, f1-score-micro: 82.11, f1-score-weighted: 82.20
----------
Epoch 25/99, current lr=0.0001
train loss: 0.004848, val loss: 0.699178, f1-score-micro: 85.65, f1-score-weighted: 85.76
----------
Epoch 26/99, current lr=0.0001
train loss: 0.007025, val loss: 0.840526, f1-score-micro: 83.08, f1-score-weighted: 83.04
----------
Epoch 27/99, current lr=0.0001
train loss: 0.005631, val loss: 0.794872, f1-score-micro: 84.62, f1-score-weighted: 84.75
----------
Epoch 28/99, current lr=0.0001
Loading best model weights!
train loss: 0.005617, val loss: 0.847493, f1-score-micro: 83.64, f1-score-weighted: 83.56
----------
Epoch 29/99, current lr=5e-05
train loss: 0.145506, val loss: 0.417631, f1-score-micro: 81.58, f1-score-weighted: 80.95
----------
Epoch 30/99, current lr=5e-05
train loss: 0.115694, val loss: 0.495550, f1-score-micro: 83.28, f1-score-weighted: 81.82
----------
Epoch 31/99, current lr=5e-05
train loss: 0.114096, val loss: 0.482025, f1-score-micro: 86.35, f1-score-weighted: 85.37
----------
Epoch 32/99, current lr=5e-05
train loss: 0.079704, val loss: 0.447818, f1-score-micro: 86.03, f1-score-weighted: 85.70
----------
Epoch 33/99, current lr=5e-05
train loss: 0.057722, val loss: 0.498897, f1-score-micro: 81.29, f1-score-weighted: 81.08
----------
Epoch 34/99, current lr=5e-05
train loss: 0.055743, val loss: 0.504095, f1-score-micro: 85.45, f1-score-weighted: 85.09
----------
Epoch 35/99, current lr=5e-05
train loss: 0.040204, val loss: 0.566408, f1-score-micro: 85.12, f1-score-weighted: 84.44
----------
Epoch 36/99, current lr=5e-05
train loss: 0.036127, val loss: 0.613605, f1-score-micro: 84.99, f1-score-weighted: 83.98
----------
Epoch 37/99, current lr=5e-05
train loss: 0.031337, val loss: 0.582721, f1-score-micro: 85.49, f1-score-weighted: 84.84
----------
Epoch 38/99, current lr=5e-05
train loss: 0.029289, val loss: 0.608272, f1-score-micro: 81.91, f1-score-weighted: 81.55
----------
Epoch 39/99, current lr=5e-05
train loss: 0.021602, val loss: 0.551165, f1-score-micro: 84.35, f1-score-weighted: 84.17
----------
Epoch 40/99, current lr=5e-05
train loss: 0.022328, val loss: 0.592862, f1-score-micro: 79.32, f1-score-weighted: 80.28
----------
Epoch 41/99, current lr=5e-05
train loss: 0.015257, val loss: 0.649402, f1-score-micro: 81.93, f1-score-weighted: 81.78
----------
Epoch 42/99, current lr=5e-05
train loss: 0.012018, val loss: 0.658397, f1-score-micro: 83.04, f1-score-weighted: 82.83
----------
Epoch 43/99, current lr=5e-05
train loss: 0.012371, val loss: 0.731281, f1-score-micro: 79.26, f1-score-weighted: 79.83
----------
Epoch 44/99, current lr=5e-05
train loss: 0.009859, val loss: 0.747663, f1-score-micro: 81.69, f1-score-weighted: 81.15
----------
Epoch 45/99, current lr=5e-05
train loss: 0.010888, val loss: 0.724683, f1-score-micro: 81.34, f1-score-weighted: 81.54
----------
Epoch 46/99, current lr=5e-05
train loss: 0.006876, val loss: 0.738027, f1-score-micro: 81.48, f1-score-weighted: 81.39
----------
Epoch 47/99, current lr=5e-05
train loss: 0.008697, val loss: 0.739757, f1-score-micro: 84.38, f1-score-weighted: 84.06
----------
Epoch 48/99, current lr=5e-05
train loss: 0.006471, val loss: 0.792608, f1-score-micro: 81.14, f1-score-weighted: 81.20
----------
Epoch 49/99, current lr=5e-05
Loading best model weights!
train loss: 0.006676, val loss: 0.776109, f1-score-micro: 82.31, f1-score-weighted: 82.37
----------
Epoch 50/99, current lr=2.5e-05
train loss: 0.153979, val loss: 0.427571, f1-score-micro: 84.69, f1-score-weighted: 82.74
----------
Epoch 51/99, current lr=2.5e-05
train loss: 0.125923, val loss: 0.427133, f1-score-micro: 82.28, f1-score-weighted: 81.57
----------
Epoch 52/99, current lr=2.5e-05
train loss: 0.102301, val loss: 0.380099, f1-score-micro: 80.55, f1-score-weighted: 80.78
----------
Epoch 53/99, current lr=2.5e-05
train loss: 0.092644, val loss: 0.370616, f1-score-micro: 87.07, f1-score-weighted: 86.67
----------
Epoch 54/99, current lr=2.5e-05
train loss: 0.081993, val loss: 0.412557, f1-score-micro: 79.91, f1-score-weighted: 80.00
----------
Epoch 55/99, current lr=2.5e-05
train loss: 0.071545, val loss: 0.449171, f1-score-micro: 85.84, f1-score-weighted: 85.47
----------
Epoch 56/99, current lr=2.5e-05
train loss: 0.065373, val loss: 0.497725, f1-score-micro: 83.34, f1-score-weighted: 82.56
----------
Epoch 57/99, current lr=2.5e-05
train loss: 0.064639, val loss: 0.438301, f1-score-micro: 84.79, f1-score-weighted: 84.59
----------
Epoch 58/99, current lr=2.5e-05
train loss: 0.049989, val loss: 0.450869, f1-score-micro: 79.90, f1-score-weighted: 79.92
----------
Epoch 59/99, current lr=2.5e-05
train loss: 0.044135, val loss: 0.503339, f1-score-micro: 82.35, f1-score-weighted: 82.50
----------
Epoch 60/99, current lr=2.5e-05
train loss: 0.042365, val loss: 0.525124, f1-score-micro: 80.81, f1-score-weighted: 80.77
----------
Epoch 61/99, current lr=2.5e-05
train loss: 0.035826, val loss: 0.519902, f1-score-micro: 84.14, f1-score-weighted: 83.88
----------
Epoch 62/99, current lr=2.5e-05
train loss: 0.032019, val loss: 0.472295, f1-score-micro: 83.44, f1-score-weighted: 83.38
----------
Epoch 63/99, current lr=2.5e-05
train loss: 0.028868, val loss: 0.563307, f1-score-micro: 78.60, f1-score-weighted: 78.75
----------
Epoch 64/99, current lr=2.5e-05
train loss: 0.028147, val loss: 0.590089, f1-score-micro: 83.38, f1-score-weighted: 82.66
----------
Epoch 65/99, current lr=2.5e-05
train loss: 0.020850, val loss: 0.608257, f1-score-micro: 79.53, f1-score-weighted: 79.53
----------
Epoch 66/99, current lr=2.5e-05
train loss: 0.021115, val loss: 0.621245, f1-score-micro: 83.73, f1-score-weighted: 83.53
----------
Epoch 67/99, current lr=2.5e-05
train loss: 0.021623, val loss: 0.610276, f1-score-micro: 83.50, f1-score-weighted: 82.75
----------
Epoch 68/99, current lr=2.5e-05
train loss: 0.016387, val loss: 0.650688, f1-score-micro: 80.71, f1-score-weighted: 80.54
----------
Epoch 69/99, current lr=2.5e-05
train loss: 0.015202, val loss: 0.660735, f1-score-micro: 80.59, f1-score-weighted: 80.43
----------
Epoch 70/99, current lr=2.5e-05
Loading best model weights!
train loss: 0.018654, val loss: 0.675118, f1-score-micro: 83.27, f1-score-weighted: 82.61
----------
Epoch 71/99, current lr=1.25e-05
train loss: 0.147401, val loss: 0.413732, f1-score-micro: 85.86, f1-score-weighted: 85.00
----------
Epoch 72/99, current lr=1.25e-05
train loss: 0.129647, val loss: 0.396034, f1-score-micro: 83.00, f1-score-weighted: 82.55
----------
Epoch 73/99, current lr=1.25e-05
Copied best model weights!
train loss: 0.111100, val loss: 0.352961, f1-score-micro: 84.85, f1-score-weighted: 84.31
----------
Epoch 74/99, current lr=1.25e-05
train loss: 0.104987, val loss: 0.404496, f1-score-micro: 81.37, f1-score-weighted: 81.06
----------
Epoch 75/99, current lr=1.25e-05
train loss: 0.100177, val loss: 0.407787, f1-score-micro: 82.28, f1-score-weighted: 82.11
----------
Epoch 76/99, current lr=1.25e-05
train loss: 0.089438, val loss: 0.412965, f1-score-micro: 82.40, f1-score-weighted: 81.97
----------
Epoch 77/99, current lr=1.25e-05
train loss: 0.091294, val loss: 0.421718, f1-score-micro: 85.49, f1-score-weighted: 84.87
----------
Epoch 78/99, current lr=1.25e-05
train loss: 0.079852, val loss: 0.426561, f1-score-micro: 83.75, f1-score-weighted: 83.24
----------
Epoch 79/99, current lr=1.25e-05
train loss: 0.079008, val loss: 0.437343, f1-score-micro: 83.38, f1-score-weighted: 83.04
----------
Epoch 80/99, current lr=1.25e-05
train loss: 0.070685, val loss: 0.418021, f1-score-micro: 84.38, f1-score-weighted: 84.09
----------
Epoch 81/99, current lr=1.25e-05
train loss: 0.067537, val loss: 0.409369, f1-score-micro: 82.72, f1-score-weighted: 82.54
----------
Epoch 82/99, current lr=1.25e-05
train loss: 0.067391, val loss: 0.426568, f1-score-micro: 86.35, f1-score-weighted: 86.12
----------
Epoch 83/99, current lr=1.25e-05
train loss: 0.064813, val loss: 0.483360, f1-score-micro: 87.13, f1-score-weighted: 86.71
----------
Epoch 84/99, current lr=1.25e-05
train loss: 0.061089, val loss: 0.361701, f1-score-micro: 83.36, f1-score-weighted: 83.25
----------
Epoch 85/99, current lr=1.25e-05
train loss: 0.047630, val loss: 0.481971, f1-score-micro: 86.36, f1-score-weighted: 85.99
----------
Epoch 86/99, current lr=1.25e-05
train loss: 0.044821, val loss: 0.483383, f1-score-micro: 83.72, f1-score-weighted: 83.46
----------
Epoch 87/99, current lr=1.25e-05
train loss: 0.047530, val loss: 0.492750, f1-score-micro: 81.10, f1-score-weighted: 80.99
----------
Epoch 88/99, current lr=1.25e-05
train loss: 0.046007, val loss: 0.495800, f1-score-micro: 82.52, f1-score-weighted: 83.02
----------
Epoch 89/99, current lr=1.25e-05
train loss: 0.044479, val loss: 0.471514, f1-score-micro: 85.86, f1-score-weighted: 85.71
----------
Epoch 90/99, current lr=1.25e-05
train loss: 0.046976, val loss: 0.502020, f1-score-micro: 86.58, f1-score-weighted: 86.06
----------
Epoch 91/99, current lr=1.25e-05
train loss: 0.050389, val loss: 0.447472, f1-score-micro: 83.33, f1-score-weighted: 83.24
----------
Epoch 92/99, current lr=1.25e-05
train loss: 0.039184, val loss: 0.539578, f1-score-micro: 81.83, f1-score-weighted: 81.29
----------
Epoch 93/99, current lr=1.25e-05
train loss: 0.037567, val loss: 0.543701, f1-score-micro: 83.88, f1-score-weighted: 83.65
----------
Epoch 94/99, current lr=1.25e-05
Loading best model weights!
train loss: 0.031056, val loss: 0.540908, f1-score-micro: 80.51, f1-score-weighted: 80.79
----------
Epoch 95/99, current lr=6.25e-06
train loss: 0.103311, val loss: 0.403642, f1-score-micro: 83.08, f1-score-weighted: 82.61
----------
Epoch 96/99, current lr=6.25e-06
train loss: 0.103533, val loss: 0.400487, f1-score-micro: 82.13, f1-score-weighted: 81.79
----------
Epoch 97/99, current lr=6.25e-06
train loss: 0.101725, val loss: 0.394552, f1-score-micro: 82.34, f1-score-weighted: 82.13
----------
Epoch 98/99, current lr=6.25e-06
train loss: 0.094731, val loss: 0.439477, f1-score-micro: 82.37, f1-score-weighted: 81.89
----------
Epoch 99/99, current lr=6.25e-06
train loss: 0.092423, val loss: 0.383869, f1-score-micro: 84.57, f1-score-weighted: 84.57
----------
Epoch 00029: reducing learning rate of group 0 to 5.0000e-05.
Epoch 00050: reducing learning rate of group 0 to 2.5000e-05.
Epoch 00071: reducing learning rate of group 0 to 1.2500e-05.
Epoch 00095: reducing learning rate of group 0 to 6.2500e-06.
